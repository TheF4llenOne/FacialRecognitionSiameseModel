{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johnn\\miniconda3\\lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.12.2 when it was built against 1.12.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import uuid library to generate unique image names\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the directories\n",
    "os.makedirs(POS_PATH, exist_ok=True)\n",
    "os.makedirs(NEG_PATH, exist_ok=True)\n",
    "os.makedirs(ANC_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lfw/Vladimir_Putin/Vladimir_Putin_0008.jpg: Can't unlink already-existing object\n",
      "tar: Error exit delayed from previous errors.\n"
     ]
    }
   ],
   "source": [
    "# http://vis-www.cs.umass.edu/lfw/\n",
    "\n",
    "# Uncompress Tar GZ Labelled Faces in the Wild Dataset\n",
    "!tar -xf lfw.tgz\n",
    "\n",
    "# Move LFW Images to the following repository data/negative\n",
    "for directory in os.listdir('lfw'):\n",
    "    for file in os.listdir(os.path.join('lfw', directory)):\n",
    "        EX_PATH = os.path.join('lfw', directory, file)\n",
    "        NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "        os.replace(EX_PATH, NEW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\anchor\\\\4e89d79f-844d-11ef-91c8-04421a97446c.jpg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened(): \n",
    "    ret, frame = cap.read()\n",
    "   \n",
    "    # Cut down frame to 250x250px\n",
    "    frame = frame[120:120+250,200:200+250, :]\n",
    "    \n",
    "    # Collect anchors \n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        # Create the unique file path \n",
    "        imgname = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        # Write out anchor image\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Collect positives\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        # Create the unique file path \n",
    "        imgname = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        # Write out positive image\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Show image back to screen\n",
    "    cv2.imshow('Image Collection', frame)\n",
    "    \n",
    "    # Breaking gracefully\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "# Release the webcam\n",
    "cap.release()\n",
    "# Close the image show frame\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mframe\u001b[49m[\u001b[38;5;241m120\u001b[39m:\u001b[38;5;241m120\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m250\u001b[39m,\u001b[38;5;241m200\u001b[39m:\u001b[38;5;241m200\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m250\u001b[39m, :])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'frame' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(frame[120:120+250,200:200+250, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(img):\n",
    "    data = []\n",
    "    for i in range(9):\n",
    "        img = tf.image.stateless_random_brightness(img, max_delta=0.02, seed=(1,2))\n",
    "        img = tf.image.stateless_random_contrast(img, lower=0.6, upper=1, seed=(1,3))\n",
    "        # img = tf.image.stateless_random_crop(img, size=(20,20,3), seed=(1,2))\n",
    "        img = tf.image.stateless_random_flip_left_right(img, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_jpeg_quality(img, min_jpeg_quality=90, max_jpeg_quality=100, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_saturation(img, lower=0.9,upper=1, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "            \n",
    "        data.append(img)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join(ANC_PATH, '6bb81fcc-82ab-11ef-91e3-04421a97446c.jpg')\n",
    "img = cv2.imread(img_path)\n",
    "augmented_images = data_aug(img)\n",
    "\n",
    "for image in augmented_images:\n",
    "    cv2.imwrite(os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1())), image.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(os.path.join(POS_PATH)):\n",
    "    img_path = os.path.join(POS_PATH, file_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    augmented_images = data_aug(img) \n",
    "    \n",
    "    for image in augmented_images:\n",
    "        cv2.imwrite(os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1())), image.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = tf.data.Dataset.list_files(ANC_PATH+'\\*.jpg').take(3000)\n",
    "positive = tf.data.Dataset.list_files(POS_PATH+'\\*.jpg').take(3000)\n",
    "negative = tf.data.Dataset.list_files(NEG_PATH+'\\*.jpg').take(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_test = anchor.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data\\\\anchor\\\\8aa2511c-82ab-11ef-a4c9-04421a97446c.jpg'\n"
     ]
    }
   ],
   "source": [
    "print(dir_test.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing - Scale and Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    \n",
    "    # Read in image from file path\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    # Load in the image \n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    \n",
    "    # Preprocessing steps - resizing the image to be 100x100x3\n",
    "    img = tf.image.resize(img, (100,100))\n",
    "    # Scale image to be between 0 and 1 \n",
    "    img = img / 255.0\n",
    "\n",
    "    # Return image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess('data\\\\anchor\\\\6bb81fcc-82ab-11ef-91e3-04421a97446c.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012254902"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'data\\\\anchor\\\\8aa2ed56-82ab-11ef-b56f-04421a97446c.jpg',\n",
       " b'data\\\\positive\\\\568c022e-844d-11ef-9373-04421a97446c.jpg',\n",
       " 1.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    return(preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = preprocess_twin(*example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23460f11310>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloader pipeline\n",
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training partition\n",
    "train_data = data.take(round(len(data)*.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training data into separate variables\n",
    "X_train_a = []\n",
    "X_train_b = []\n",
    "y_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the training dataset to extract data\n",
    "for batch in train_data.as_numpy_iterator():\n",
    "    x_a, x_b, labels = batch\n",
    "    X_train_a.extend(x_a)  # Collect anchor images\n",
    "    X_train_b.extend(x_b)  # Collect positive/negative images\n",
    "    y_train.extend(labels)  # Collect labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to NumPy arrays\n",
    "X_train_a = np.array(X_train_a)\n",
    "X_train_b = np.array(X_train_b)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing partition\n",
    "test_data = data.skip(round(len(data)*.7))\n",
    "test_data = test_data.take(round(len(data)*.3))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Architecture and Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base network\n",
    "def create_base_network():\n",
    "    inputs = Input(shape=(100,100,3), name='input_image')\n",
    "\n",
    "    # Layer 1\n",
    "    conv1 = Conv2D(64, (10, 10), activation='relu')(inputs)\n",
    "    pool1 = MaxPooling2D(64, (2,2), padding='same')(conv1)\n",
    "\n",
    "    # Layer 2\n",
    "    conv2 = Conv2D(128, (7, 7), activation='relu')(pool1)\n",
    "    pool2 = MaxPooling2D(64, (2,2), padding='same')(conv2)\n",
    "\n",
    "    # Layer 3\n",
    "    conv3 = Conv2D(128, (4, 4), activation='relu')(pool2)\n",
    "    pool3 = MaxPooling2D(64, (2,2), padding='same')(conv3)\n",
    "\n",
    "    # Layer 4\n",
    "    conv4 = Conv2D(256, (4, 4), activation='relu')(pool3)\n",
    "    \n",
    "    # Flatten the output of the last convolutional layer\n",
    "    flat = Flatten()(conv4)\n",
    "\n",
    "    # Fully connected layer\n",
    "    dense1 = Dense(4096, activation='sigmoid')(flat)\n",
    "    drop = Dropout(0.2)(dense1) \n",
    "\n",
    "    # Define the base model\n",
    "    base_network = Model(inputs=inputs, outputs=dense1)\n",
    "    base_network.summary()\n",
    "\n",
    "    return Model(inputs=inputs, outputs=drop, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 91, 91, 64)        19264     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 46, 46, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 40, 40, 128)       401536    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 20, 20, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 17, 17, 128)       262272    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 9, 9, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 256)         524544    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the base network\n",
    "base_network = create_base_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model(base_network):\n",
    "    # Define the inputs for the Siamese Network\n",
    "    input_a = Input(name='input_img', shape=(100, 100, 3))\n",
    "    input_b = Input(name='validation_img', shape=(100, 100, 3))\n",
    "\n",
    "    # Process both inputs using the shared base network\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "\n",
    "    # Define a Lambda layer for computing the absolute difference between the two embeddings\n",
    "    # This will serve as the similarity measure between the two inputs\n",
    "    distance = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))([processed_a, processed_b])\n",
    "\n",
    "    # Define the fully connected layers and output layer for similarity scoring\n",
    "    classifier = Dense(1, activation='sigmoid')(distance)\n",
    "    \n",
    "    # Create the Siamese model\n",
    "    model = Model(inputs=[input_a, input_b], outputs=classifier, name='SiameseNetwork')\n",
    "\n",
    "    # Compile the model with a binary cross-entropy loss (since we are dealing with a similarity score)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Print the model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Return the Siamese model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_img (InputLayer)         [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " validation_img (InputLayer)    [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)         (None, 4096)         38960448    ['input_img[0][0]',              \n",
      "                                                                  'validation_img[0][0]']         \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 4096)         0           ['embedding[0][0]',              \n",
      "                                                                  'embedding[1][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            4097        ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,964,545\n",
      "Trainable params: 38,964,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model = make_siamese_model(base_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoints and early stopping\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# Create checkpoint directory if it doesn't exist\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Create the checkpoint for saving the model and optimizer state\n",
    "checkpoint = tf.train.Checkpoint(optimizer=opt, siamese_model=siamese_model)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the Siamese Network\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43msiamese_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_b\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                            \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust the number of epochs as necessary\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Save the model and optimizer state after training\u001b[39;00m\n\u001b[0;32m     10\u001b[0m checkpoint\u001b[38;5;241m.\u001b[39msave(file_prefix\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mckpt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\johnn\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\johnn\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\johnn\\miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\johnn\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\johnn\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:980\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    977\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;66;03m# stateless function.\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    983\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    984\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n",
      "File \u001b[1;32mc:\\Users\\johnn\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\johnn\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\johnn\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\johnn\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the Siamese Network\n",
    "history = siamese_model.fit([X_train_a, X_train_b], \n",
    "                            y_train,\n",
    "                            batch_size=16,\n",
    "                            epochs=50,  # Adjust the number of epochs as necessary\n",
    "                            validation_split=0.2,\n",
    "                            callbacks=[early_stopping])\n",
    "\n",
    "# Save the model and optimizer state after training\n",
    "checkpoint.save(file_prefix=os.path.join(checkpoint_dir, 'ckpt'))\n",
    "\n",
    "siamese_model.save('siamesemodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model using custom_objects with the Lambda function or K.abs directly\n",
    "siamese_model = tf.keras.models.load_model('siamesemodel.h5',\n",
    "    custom_objects={'<lambda>': Lambda(lambda tensors: K.abs(tensors[0] - tensors[1])), \n",
    "                    'BinaryCrossentropy': tf.losses.BinaryCrossentropy}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test data\n",
    "test_input, test_val, y_true = test_data.as_numpy_iterator().next()\n",
    "y_hat = siamese_model.predict([test_input, test_val])\n",
    "\n",
    "# Post processing the results \n",
    "[1 if prediction > 0.5 else 0 for prediction in y_hat ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a metric object \n",
    "m = Recall()\n",
    "\n",
    "# Calculating the recall value \n",
    "m.update_state(y_true, y_hat)\n",
    "\n",
    "# Return Recall Result\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a metric object \n",
    "m = Precision()\n",
    "\n",
    "# Calculating the recall value \n",
    "m.update_state(y_true, y_hat)\n",
    "\n",
    "# Return Recall Result\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Recall()\n",
    "p = Precision()\n",
    "\n",
    "for test_input, test_val, y_true in test_data.as_numpy_iterator():\n",
    "    yhat = siamese_model.predict([test_input, test_val])\n",
    "    r.update_state(y_true, yhat)\n",
    "    p.update_state(y_true,yhat) \n",
    "\n",
    "print(r.result().numpy(), p.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Recall()\n",
    "p = Precision()\n",
    "\n",
    "for test_input, test_val, y_true in test_data.as_numpy_iterator():\n",
    "    yhat = siamese_model.predict([test_input, test_val])\n",
    "    r.update_state(y_true, yhat)\n",
    "    p.update_state(y_true,yhat) \n",
    "\n",
    "print(r.result().numpy(), p.result().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
